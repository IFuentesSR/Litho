{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import re\n",
    "from urllib import request\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####base url and apikey\n",
    "#### it requires an apiKey from Elsevier\n",
    "#doi = '10.1016/j.enggeo.2012.07.017'\n",
    "apiKey = 'replace with Elsevier apiKey'\n",
    "base = 'https://api.elsevier.com/content/article/doi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieveTextPaper(url, apikey, doi):\n",
    "    params = {\n",
    "        'APIKey': apikey,\n",
    "        'view': 'FULL',\n",
    "        'httpAccept': 'text/xml'  #### application/json    text/xml\n",
    "        }\n",
    "    post_args = urllib.parse.urlencode(params)\n",
    "\n",
    "    test = request.urlopen(url+doi+'?'+post_args)\n",
    "    html = test.read()\n",
    "    save()\n",
    "\n",
    "    html = ET.fromstring(html)\n",
    "    paragraphs = []\n",
    "\n",
    "    for n in html:\n",
    "        if 'originalText' in n.tag:\n",
    "            OT = n\n",
    "            break\n",
    "\n",
    "    for n in OT:\n",
    "        if 'doc' in n.tag:\n",
    "            doc = n\n",
    "            break\n",
    "\n",
    "    for n in doc:\n",
    "        if 'serial' in n.tag:\n",
    "            serial = n\n",
    "            break\n",
    "\n",
    "    for n in serial:\n",
    "        if 'article' in n.tag:\n",
    "            article = n\n",
    "            break\n",
    "\n",
    "    for n in article:\n",
    "        if 'body' in n.tag:\n",
    "            body = n\n",
    "            break\n",
    "\n",
    "    for n in body:\n",
    "        if 'sections' in n.tag:\n",
    "            sections = n\n",
    "            break\n",
    "\n",
    "    for n in sections:\n",
    "        for m in n:\n",
    "            if 'para' in m.tag:\n",
    "                # TODO: Remove refs, tables, and eq.\n",
    "                paragraphs.append(''.join(m.itertext()))\n",
    "\n",
    "    text1 = ''.join(paragraphs)\n",
    "    return text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####finding dois of papers to use\n",
    "\n",
    "##base url\n",
    "base2 = 'https://api.elsevier.com/content/search/scidir'\n",
    "\n",
    "def getDOIs(url, query, apikey, subscribed='true', counts = 200, year = None):\n",
    "    dois = []\n",
    "    if year is None:\n",
    "        for n in range(1, 5801,200):\n",
    "            try:\n",
    "                if n == 1:\n",
    "                    param2 = {\n",
    "                        'query': '{} AND pub-date IS {}'.format(query, year),\n",
    "                        'APIKey': apikey,\n",
    "                        'subscribed': subscribed,\n",
    "                        'count': counts\n",
    "                    }\n",
    "                else:\n",
    "                    param2 = {\n",
    "                        'query': '{} AND pub-date IS {}'.format(query, year),\n",
    "                        'APIKey': apikey,\n",
    "                        'subscribed': subscribed,\n",
    "                        'count': counts,\n",
    "                        'start': str(n)\n",
    "                    }\n",
    "                post_args2 = urllib.parse.urlencode(param2)\n",
    "\n",
    "                newTest = request.urlopen(base2+'?'+post_args2)\n",
    "                html = newTest.read()\n",
    "                for z in json.loads(html.decode(\"utf-8\"))['search-results']['entry']:\n",
    "                    dois.append(z['dc:identifier'][4:])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "    else:\n",
    "        year = year\n",
    "        param2 = {\n",
    "            'query': '{} AND pub-date IS {}'.format(query, year),\n",
    "            'APIKey': apikey,\n",
    "            'subscribed': subscribed,\n",
    "            'count': counts\n",
    "            }\n",
    "        post_args2 = urllib.parse.urlencode(param2)\n",
    "        newTest = request.urlopen(base2+'?'+post_args2)\n",
    "        html = newTest.read()\n",
    "        la = json.loads(html.decode(\"utf-8\"))['search-results']\n",
    "        number = int(la['opensearch:totalResults'])\n",
    "        print(year, number)\n",
    "        for n in range(1,number,200):\n",
    "            if n == 1:\n",
    "                param2 = {\n",
    "                    'query': '{} AND pub-date IS {}'.format(query, year),\n",
    "                    'APIKey': apikey,\n",
    "                    'subscribed': subscribed,\n",
    "                    'count': counts\n",
    "                    }\n",
    "            else:\n",
    "                param2 = {\n",
    "                    'query': '{} AND pub-date IS {}'.format(query, year),\n",
    "                    'APIKey': apikey,\n",
    "                    'subscribed': subscribed,\n",
    "                    'count': counts,\n",
    "                    'start': n\n",
    "                    }\n",
    "            post_args2 = urllib.parse.urlencode(param2)\n",
    "            newTest = request.urlopen(base2+'?'+post_args2)\n",
    "            html = newTest.read()\n",
    "            for z in json.loads(html.decode(\"utf-8\"))['search-results']['entry']:\n",
    "                dois.append(z['dc:identifier'][4:])\n",
    "\n",
    "    return dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 796\n",
      "2001 834\n",
      "2002 910\n",
      "2003 974\n",
      "2004 965\n",
      "2005 1052\n",
      "2006 1276\n",
      "2007 1262\n",
      "2008 1413\n",
      "2009 1611\n",
      "2010 1604\n",
      "2011 1580\n",
      "2012 1925\n",
      "2013 2185\n",
      "2014 2445\n",
      "2015 2683\n",
      "2016 2909\n",
      "2017 3279\n",
      "2018 2060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31744"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####to get all DOIs from 2000, to 2018 using the keyword lithology\n",
    "allDOI = []\n",
    "for n in range(2000, 2019, 1):\n",
    "    DOIs = getDOIs(base2, 'lithology', apiKey, year = n)\n",
    "    allDOI.append(DOIs)\n",
    "\n",
    "allDOI = [item for sublist in allDOI for item in sublist]\n",
    "len(allDOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##saving DOI list as picke object\n",
    "with open('allDOI.pickle', 'wb') as f:\n",
    "    pickle.dump(allDOI, f)\n",
    "##importing DOI list saved\n",
    "with open('allDOI.pickle', 'rb') as f:\n",
    "    tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##to download all papers from DOIs list as txt files into the paper folder\n",
    "##The number of papers is over 30,000, so it takes a while to finish!\n",
    "\n",
    "for n in tmp:\n",
    "    try:\n",
    "        doi = n\n",
    "        content = retrieveTextPaper(url = base, apikey = apiKey, doi=doi)\n",
    "        if len(content) != 0:\n",
    "            with open('papers/{}.txt'.format(doi.replace('/', '-')), 'w') as files:\n",
    "                files.write(content)\n",
    "                files.close\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
