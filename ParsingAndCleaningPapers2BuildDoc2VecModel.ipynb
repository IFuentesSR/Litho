{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = os.getcwd()+'/'\n",
    "paperFolder = 'papers/'\n",
    "stopw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(path, text0, stopwords):\n",
    "    ##returns a list of lists, with words in it.\n",
    "    with open(path+text0) as txt:\n",
    "        data = txt.read()\n",
    "\n",
    "        # data = re.sub('\\\\w+\\\\set\\\\sal', '<UNK>', data)\n",
    "        data = re.sub('\\\\w+\\\\sand\\\\s+\\\\w+\\s\\\\(\\\\d{4}\\\\)', '<UNK>', data)\n",
    "        data = re.sub('[A-Z][a-z]*\\s+\\\\(\\\\d{4}\\\\)', '<UNK>', data)\n",
    "        data = re.sub('\\\\([^)]*\\\\)', '<UNK>', data)\n",
    "        data = re.sub('[A-Z][a-z]*\\\\set\\\\sal.', '<UNK>', data)\n",
    "        data = re.sub('\\s+[.]', '.', data)\n",
    "        data = re.sub('\\s+[,]', ',', data)\n",
    "        data = data.replace(u'\\xa0', u' ')\n",
    "        data = data.split('\\n')\n",
    "        data = [n.strip().split('.') for n in data]\n",
    "        data = [[z.strip().split(' ') for z in n] for n in data]\n",
    "        data = [[z for z in n if len(z) > 5] for n in data if len(n) > 0]\n",
    "        data = [n for n in data if len(n) > 0]\n",
    "        data = [[n for sublist in l for n in sublist] for l in data]\n",
    "        data = [[n.lower() for n in l if not re.search('\\\\d', n)] for l in data]\n",
    "        data = [[n for n in l if re.search('[a-zA-Z]', n)] for l in data] ####TODO check\n",
    "        data = [[n for n in l if n not in stopwords] for l in data]\n",
    "        data = [[n for n in l if '<unk>' not in n] for l in data]\n",
    "        data = [[n for n in l if len(n)>1] for l in data]\n",
    "        data = [[n.replace(',','') for n in l] for l in data]\n",
    "        data = [[n.replace(\"'s\",'') for n in l] for l in data]\n",
    "        # data = [[n.split('-') for n in l] for l in data]\n",
    "        # data = [[n for n in l if not re.search('\\\\', n)] for l in data]\n",
    "        txt.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def labeledSentences(tokenizedDescriptions, labels = None):\n",
    "    sentences = [gensim.models.doc2vec.TaggedDocument(words = i, tags = [labels[n]]) for n, i in enumerate(tokenizedDescriptions)]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Developing Model\n",
    "trainList = np.random.choice(os.listdir(root+paperFolder), 1000, replace = False).tolist()\n",
    "trainPapers = [clean(root+paperFolder, n, stopw) for n in trainList]\n",
    "\n",
    "##labels\n",
    "labels = list(range(np.sum([len(n) for n in trainPapers])))\n",
    "labels = ['lab_{}'.format(n) for n in labels]\n",
    "len(labels)\n",
    "\n",
    "##sentences\n",
    "sentences = []\n",
    "for n in trainPapers:\n",
    "    for m in n:\n",
    "        sentences.append(m)\n",
    "len(sentences)\n",
    "\n",
    "\n",
    "## Doc2Vec Model\n",
    "labSen = labeledSentences(sentences, labels = labels)\n",
    "\n",
    "model = gensim.models.Doc2Vec(size = 100,\n",
    "                              window = 3,\n",
    "                              sample = 1e-3,\n",
    "                              min_count = 1,\n",
    "                              workers = 8,\n",
    "                              seed = 1,\n",
    "                              dm_concat = 1,\n",
    "                              dm_mean = 1,\n",
    "                              dm = 1)\n",
    "model.build_vocab(labSen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
