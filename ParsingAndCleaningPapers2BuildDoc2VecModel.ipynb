{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ifue3702\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from six import iteritems\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ifue3702\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from Litho.nlp_funcs import *\n",
    "from Litho.similarity import (check_similarity, match_lithcode, jaccard_similarity, \n",
    "                              calc_similarity_score, print_sim_compare, merge_similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "paperFolder = 'papers'\n",
    "stopa = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsingPapers(path, text0, stopwords):\n",
    "    ##returns a list of lists, with words in it.\n",
    "    with open(os.path.join(path,text0)) as txt:\n",
    "        data = txt.read()\n",
    "\n",
    "        # data = re.sub('\\\\w+\\\\set\\\\sal', '<UNK>', data)\n",
    "        data = re.sub('\\\\w+\\\\sand\\\\s+\\\\w+\\s\\\\(\\\\d{4}\\\\)', '<UNK>', data)\n",
    "        data = re.sub('[A-Z][a-z]*\\s+\\\\(\\\\d{4}\\\\)', '<UNK>', data)\n",
    "        data = re.sub('\\\\([^)]*\\\\)', '<UNK>', data)\n",
    "        data = re.sub('[A-Z][a-z]*\\\\set\\\\sal.', '<UNK>', data)\n",
    "        data = re.sub('\\s+[.]', '.', data)\n",
    "        data = re.sub('\\s+[,]', ',', data)\n",
    "        data = data.replace(u'\\xa0', u' ')\n",
    "        data = data.split('\\\\n')\n",
    "        data = [n.strip().split('.') for n in data]\n",
    "        data = [[z.strip().split(' ') for z in n] for n in data]\n",
    "        data = [[z for z in n if len(z) > 5] for n in data if len(n) > 0]\n",
    "        data = [n for n in data if len(n) > 0]\n",
    "        data = [n for n in [x for y in data for x in y]]\n",
    "        data = [[n.lower() for n in l if not re.search('\\\\d', n)] for l in data]\n",
    "        data = [[n.replace(',','') for n in l] for l in data]\n",
    "        data = [[n for n in l if n.isalpha()] for l in data] ####TODO check\n",
    "        data = [[n for n in l if n not in stopwords] for l in data]\n",
    "        data = [[n for n in l if '<unk>' not in n] for l in data]\n",
    "        data = [[n for n in l if len(n)>2] for l in data]\n",
    "#         data = [[n.replace(\"'s\",'') for n in l] for l in data]\n",
    "        # data = [[n.split('-') for n in l] for l in data]\n",
    "        # data = [[n for n in l if not re.search('\\\\', n)] for l in data]\n",
    "        txt.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def labeledSentences(tokenizedDescriptions, labels = None):\n",
    "    sentences = [gensim.models.doc2vec.TaggedDocument(words = i, tags = [labels[n]]) for n, i in enumerate(tokenizedDescriptions)]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers = [n for n in os.listdir(os.path.join(root, paperFolder)) if n.endswith('.txt')]\n",
    "papers = np.random.choice(papers, 10000, replace = False).tolist()\n",
    "trainPapers = [parsingPapers(os.path.join(root,paperFolder), n, stopa) for n in papers]\n",
    "trainPapers = [n for sublist in trainPapers for n in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905254"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainPapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(trainPapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flevoland\n",
      "rijp\n",
      "almere\n",
      "stichtse\n",
      "kant\n",
      "swifterbant\n",
      "disassembled\n",
      "geovalidation\n",
      "khataba\n",
      "geovalidate\n",
      "roja\n",
      "dolomiteous\n",
      "demonte\n",
      "belbo\n",
      "orba\n",
      "stura\n",
      "scheck\n",
      "subpanels\n",
      "generously\n",
      "lindsleyite\n",
      "lithos\n",
      "anis\n",
      "kubub\n",
      "hanaus\n",
      "tschermaks\n",
      "letlhakane\n",
      "genotypes\n",
      "contests\n",
      "onu\n",
      "valentina\n",
      "timbetes\n",
      "renz\n",
      "ichnocoenose\n",
      "euroharp\n",
      "nica\n",
      "autocalibrated\n",
      "funen\n",
      "redoxfront\n",
      "lanpingensis\n",
      "tectochara\n",
      "subelongta\n",
      "gyrogona\n",
      "obtusochara\n",
      "qianjiangica\n",
      "spinosalata\n",
      "sinocypris\n",
      "yunlongensis\n",
      "exselsa\n",
      "likiangensis\n",
      "eoentelodon\n",
      "lunania\n",
      "baoxingsi\n",
      "yongsheng\n",
      "tongdian\n",
      "vallenar\n",
      "totoralillo\n",
      "biese\n",
      "astrocoenia\n",
      "hexamera\n",
      "prophyritic\n",
      "ocoitas\n",
      "studer\n",
      "agria\n",
      "blumebachi\n",
      "parahopilites\n",
      "mediterraneotrigonia\n",
      "domeykanus\n",
      "parancyloceras\n",
      "curvimetric\n",
      "rotomakariri\n",
      "patiti\n",
      "ctdo\n",
      "antimonate\n",
      "thioantimony\n",
      "skuhl\n",
      "mugharan\n",
      "mochi\n",
      "tossing\n",
      "kindled\n",
      "conscientious\n",
      "glowing\n",
      "tungurahua\n",
      "ngauruhoe\n",
      "citric\n",
      "mammillated\n",
      "hallsworth\n",
      "mange\n",
      "onjeongri\n",
      "pukyong\n",
      "asrmari\n",
      "aknowledge\n",
      "sabins\n",
      "gregg\n",
      "aleks\n",
      "kalinowski\n",
      "battock\n",
      "atholl\n",
      "tummel\n",
      "muick\n",
      "portsoy\n"
     ]
    }
   ],
   "source": [
    "for n in once_ids[0:100]:\n",
    "    print(dictionary[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(80570 unique tokens: ['bradelle', 'mcd', 'mimics', 'investigaciones', 'tightened']...)\n",
      "905254\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(trainPapers)\n",
    "## newt two lines remove tokens contained just once in the dictionary\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in trainPapers]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Developing Model\n",
    "# trainList = np.random.choice(os.listdir(os.path.join(root,paperFolder)), 1000, replace = False).tolist()\n",
    "# trainPapers = [parsingPapers(root+paperFolder, n, stopw) for n in trainList]\n",
    "\n",
    "##labels\n",
    "labels = list(range(np.sum([len(n) for n in trainPapers])))\n",
    "labels = ['lab_{}'.format(n) for n in labels]\n",
    "len(labels)\n",
    "\n",
    "##sentences\n",
    "sentences = []\n",
    "for n in trainPapers:\n",
    "    for m in n:\n",
    "        sentences.append(m)\n",
    "len(sentences)\n",
    "\n",
    "\n",
    "## Doc2Vec Model\n",
    "labSen = labeledSentences(sentences, labels = labels)\n",
    "\n",
    "model = gensim.models.Doc2Vec(size = 300,\n",
    "                              window = 3,\n",
    "#                               sample = 1e-3,\n",
    "                              min_count = 1,\n",
    "                              workers = 4,\n",
    "#                               seed = 1,\n",
    "#                               dm_concat = 1,\n",
    "#                               dm_mean = 1,\n",
    "#                               dm = 1)\n",
    "                              iter = 5)\n",
    "model.build_vocab(labSen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n"
     ]
    }
   ],
   "source": [
    "#training of model\n",
    "for epoch in range(5):\n",
    "    print('iteration '+str(epoch+1))\n",
    "    model.train(labSen, total_examples=len(labSen), epochs = 5)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Developing Model\n",
    "# trainList = np.random.choice(os.listdir(root+paperFolder), 1000, replace = False).tolist()\n",
    "trainList = os.listdir(root+paperFolder)\n",
    "# trainPapers = [clean(root+paperFolder, n, stopw) for n in trainList]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopw2 = ['redish', 'reddish', 'red', 'black', 'blackish', 'brown', 'brownish',\n",
    "          'blue', 'blueish', 'orange', 'orangeish', 'gray', 'grey', 'grayish',\n",
    "          'greyish', 'white', 'whiteish', 'purple', 'purpleish', 'yellow',\n",
    "          'yellowish', 'green', 'greenish', 'light', 'very', 'pink', 'coloured', 'multicoloured',\n",
    "          'dark', 'color', 'colour', 'hard', 'soft', 'water', 'supply', 'fine', 'coarse',\n",
    "          'medium', 'bearing', 'pipe', 'sticky', 'tough', 'small', 'stiff', \n",
    "          'running', 'streaks', 'nominal', 'bands', 'back', 'slippery', 'loose', \n",
    "          'broken', 'fractured', 'surface', 'surface', 'rotten', 'compacted', 'seams',\n",
    "          'dry', 'wet', 'cemented', 'rock', 'trap', 'ridge', 'large', 'small', 'thin', 'soak',\n",
    "          'creek', 'ridge', 'clean', 'decomposed', 'band']\n",
    "\n",
    "\n",
    "stopa= stopa+stopw2  # add the additional stopwords above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127930,) (127930,)\n"
     ]
    }
   ],
   "source": [
    "#use extend so it's a big flat list of vocab\n",
    "Namoi = 'reclasified.csv'\n",
    "Data = pd.read_csv(os.path.join(root,Namoi))\n",
    "Data = Data.dropna(subset=['Description'])\n",
    "Descriptions = Data[['Description']]\n",
    "Descriptions=Descriptions.Description.tolist()\n",
    "Descriptions = [str(n) for n in Descriptions]\n",
    "lemm_total = []\n",
    "token_total = []\n",
    "for i in Descriptions:\n",
    "    allwords_stemmed = tokenize_and_lemmatize(i, stopa) #for each item in 'synopses', tokenize/stem\n",
    "    lemm_total.extend([allwords_stemmed]) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "    allwords_tokenized = tokenize_only(i, stopa)\n",
    "    token_total.extend([allwords_tokenized])\n",
    "\n",
    "print(np.shape(token_total),  np.shape(lemm_total))\n",
    "Data['words'] = [len(n) for n in token_total]\n",
    "Data['Tokens'] = token_total\n",
    "Data['Lemmas'] = lemm_total\n",
    "Data = Data[Data.words > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.443677201091 0.6660579445 0.674838016866\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "[vectors.append(np.reshape(model.infer_vector(n), (1,300))) for n in Data.Tokens]\n",
    "X = np.vstack(vectors)\n",
    "kmeans = cluster.MiniBatchKMeans(n_clusters = len(Data.second.unique())).fit(X)\n",
    "pred_classes = kmeans.predict(X)\n",
    "print(metrics.adjusted_rand_score(Data['second'].tolist(), pred_classes.tolist()),\n",
    "     metrics.adjusted_mutual_info_score(Data['second'].tolist(), pred_classes.tolist()),\n",
    "     metrics.completeness_score(Data['second'].tolist(), pred_classes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "DataSampled = Data.sample(1000)\n",
    "DataDes=DataSampled.Description.tolist()\n",
    "DataDes = [str(n) for n in DataDes]\n",
    "\n",
    "\n",
    "# #use extend so it's a big flat list of vocab\n",
    "lemmatized = []\n",
    "tokenized = []\n",
    "for i in DataDes:\n",
    "    allwords_stemmed = tokenize_and_lemmatize(i, stopa) #for each item in 'synopses', tokenize/stem\n",
    "    lemmatized.extend([allwords_stemmed]) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "    allwords_tokenized = tokenize_only(i, stopa)\n",
    "    tokenized.extend([allwords_tokenized])\n",
    "\n",
    "print(np.shape(tokenized),  np.shape(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for n, i in enumerate(Data['Tokens'].tolist()):\n",
    "    vec_bow = dictionary.doc2bow(i)\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    vector = gensim.matutils.sparse2full(vec_lsi, length = 300)\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.649789512228 0.673252481327 0.673567044358\n"
     ]
    }
   ],
   "source": [
    "Data['vecs'] = vectors\n",
    "X = np.vstack(Data.vecs)\n",
    "kmeans = MiniBatchKMeans(n_clusters = len(Data.second.unique()), batch_size = 100).fit(X)\n",
    "pred_classes = kmeans.predict(X)\n",
    "\n",
    "print(metrics.adjusted_rand_score(Data.second.tolist(), pred_classes.tolist()),\n",
    "      metrics.adjusted_mutual_info_score(Data.second.tolist(), pred_classes.tolist()), \n",
    "      metrics.completeness_score(Data.second.tolist(), pred_classes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf = gensim.models.TfidfModel(corpus, normalize = True)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for n, i in enumerate(Data['Tokens'].tolist()):\n",
    "    vec_bow = dictionary.doc2bow(i)\n",
    "    vec_lsi = tfidf[vec_bow]\n",
    "    vector = gensim.matutils.any2sparse(vec_lsi)\n",
    "    vectors.append(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-30314584b7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vecs'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msecond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ifue3702\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \"\"\"\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "Data['vecs'] = vectors\n",
    "X = np.vstack(Data.vecs)\n",
    "kmeans = MiniBatchKMeans(n_clusters = len(Data.second.unique()), batch_size = 100).fit(X)\n",
    "pred_classes = kmeans.predict(X)\n",
    "\n",
    "print(metrics.adjusted_rand_score(Data.second.tolist(), pred_classes.tolist()),\n",
    "      metrics.adjusted_mutual_info_score(Data.second.tolist(), pred_classes.tolist()), \n",
    "      metrics.completeness_score(Data.second.tolist(), pred_classes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaMulticore(corpus_tfidf, id2word=dictionary, num_topics=50, chunksize=10000, passes=2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for n, i in enumerate(Data['Tokens'].tolist()):\n",
    "    vec_bow = dictionary.doc2bow(i)\n",
    "    vec_lsi = lda[vec_bow]\n",
    "    vector = gensim.matutils.sparse2full(vec_lsi, length = 50)\n",
    "    vectors.append(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data['vecs'] = vectors\n",
    "X = np.vstack(Data.vecs)\n",
    "kmeans = MiniBatchKMeans(n_clusters = len(Data.second.unique()), batch_size = 100).fit(X)\n",
    "pred_classes = kmeans.predict(X)\n",
    "\n",
    "print(metrics.adjusted_rand_score(Data.second.tolist(), pred_classes.tolist()),\n",
    "      metrics.adjusted_mutual_info_score(Data.second.tolist(), pred_classes.tolist()), \n",
    "      metrics.completeness_score(Data.second.tolist(), pred_classes.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
